{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Kaushik Ilango\n",
    "CWID: 20011241\n",
    "\n",
    "Previously in HW1 I used the dataset as is and predicted for 5 possible labels. Now I have converted the data into binaries which means I will be doing a binary classification. For this I have converted all the ratings that are 4 and 5 to 1 and remaining to 0. Below I have also analysed some statistics for the same. As always let's import some libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the data into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "amz_rev = pd.read_csv('amazon_reviews.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I have completely used the nltk libraries to solve the problem of pre-processing the text data since it gives us the best result when doing text related tasks. I have used both the regex and stopwords to remove punctuations and stopwords from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kilan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def setup_data(corpus):\n",
    "    print(\"Cleaning and removing stopwords from data ... \")\n",
    "    cleaned_corpus = []\n",
    "    stop_words = set(stopwords.words('english'))  # Define stop words for English\n",
    "\n",
    "    for doc in corpus:\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        words = tokenizer.tokenize(doc.lower())\n",
    "        filtered_words = [word for word in words if word not in stop_words]\n",
    "        cleaned_corpus.append(filtered_words)\n",
    "\n",
    "    count = len(cleaned_corpus)\n",
    "    print(f\"Done cleaning and removing stopwords from {count} data\")\n",
    "    return cleaned_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replacing empty string to a string \"NA\" which will make it easier for me to handle them in model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "amz_rev['reviewText'].fillna('NA', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing the nltk functions to clean and pre-process the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and removing stopwords from data ... \n",
      "Done cleaning and removing stopwords from 4915 data\n"
     ]
    }
   ],
   "source": [
    "X = setup_data(amz_rev['reviewText'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is used to convert the 5 multi-class label to binary labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_binaries(corpus):\n",
    "    y = []\n",
    "    print(\"Converting to binaries ... \")\n",
    "    for k in corpus:\n",
    "        if k > 3:\n",
    "            y.append(1)\n",
    "        else:\n",
    "            y.append(0)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing the above on the 'overall' column of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to binaries ... \n"
     ]
    }
   ],
   "source": [
    "y = convert_binaries(amz_rev['overall'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is used to retrieve vocabulary used in the dataset. This is a re-used function from previous code HW1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(corpus):\n",
    "    vocab = []\n",
    "    vocab = [x for line in corpus for x in line]\n",
    "    vocab = list(set(vocab))\n",
    "    vocab = sorted(vocab)\n",
    "    return vocab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up some globals so that these can be used in training. I have set the MAX_SEQUENCE_LENGTH to 100 since it gives us a best shot between huge and medium sized text reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 100\n",
    "VOCAB_SIZE = len(get_vocab(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is now being developed to convert the text to numbers (tokenization) and since we are random embedding which will be done directly in the model we can just tokenize them and pad them using the in-build functions tensorflow provides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def get_sequences(corpus):\n",
    "    tokenizer = Tokenizer(num_words=VOCAB_SIZE)\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    sequences = tokenizer.texts_to_sequences(corpus)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "    return padded_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing the above function to store all the tokens in the below variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_seq = get_sequences(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-used function from HW1 to split dataset into train,test,val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def train_test_val_split(X,y, test_size=0.1, val_size = 0.1,random_state=16):\n",
    "    total_test_size = test_size + val_size\n",
    "    X_train, X_t, y_train, y_t = train_test_split(X, y, test_size=total_test_size, random_state=random_state)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_t, y_t, test_size=test_size/total_test_size, random_state=random_state)\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execting the above function on tokens and the labels created from to_binary function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = train_test_val_split(X_seq,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I noticed that the labels were not in numpy type but rather a list type which threw in errors during training so I have converted them into np.arrays. I could Hvae directly converted them splitting as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.array(y_test)\n",
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing layers and callbacks for early stopping of model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, MaxPooling1D, Conv1D,Flatten\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have built the model as requested in the requirements. The embeddings are randomized and hence will learnt in the first layer of the model as we train more data. The next layer is the LSTM Layer -1 with input of 128 and further reduced to 64 in layer-2 from which we use a sigmoid activation function to get a single value (0 or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_model = Sequential()\n",
    "\n",
    "RNN_model.add(Embedding(VOCAB_SIZE, output_dim = 128, input_length=MAX_SEQUENCE_LENGTH))\n",
    "RNN_model.add(LSTM(128,return_sequences=True))\n",
    "RNN_model.add(LSTM(64))\n",
    "RNN_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "RNN_model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing out the model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 128)          1071744   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100, 128)          131584    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1252801 (4.78 MB)\n",
      "Trainable params: 1252801 (4.78 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "RNN_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have this line of code for early stopping because while training I noticed that the val_loss delta was not changing that much and felt like I could stop the training after few epochs of low delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystopping = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, patience = 3,min_delta = 0.008)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the model on our dataset and running epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "31/31 [==============================] - 14s 343ms/step - loss: 0.5853 - accuracy: 0.9036 - val_loss: 0.4902 - val_accuracy: 0.9165\n",
      "Epoch 2/100\n",
      "31/31 [==============================] - 10s 329ms/step - loss: 0.4419 - accuracy: 0.9039 - val_loss: 0.3778 - val_accuracy: 0.9165\n",
      "Epoch 3/100\n",
      "31/31 [==============================] - 10s 328ms/step - loss: 0.3651 - accuracy: 0.9039 - val_loss: 0.3224 - val_accuracy: 0.9165\n",
      "Epoch 4/100\n",
      "31/31 [==============================] - 11s 342ms/step - loss: 0.3324 - accuracy: 0.9039 - val_loss: 0.2995 - val_accuracy: 0.9165\n",
      "Epoch 5/100\n",
      "31/31 [==============================] - 10s 338ms/step - loss: 0.3204 - accuracy: 0.9039 - val_loss: 0.2902 - val_accuracy: 0.9165\n",
      "Epoch 6/100\n",
      "31/31 [==============================] - 11s 345ms/step - loss: 0.3160 - accuracy: 0.9039 - val_loss: 0.2862 - val_accuracy: 0.9165\n",
      "Epoch 7/100\n",
      "31/31 [==============================] - 12s 377ms/step - loss: 0.3144 - accuracy: 0.9039 - val_loss: 0.2843 - val_accuracy: 0.9165\n",
      "Epoch 8/100\n",
      "31/31 [==============================] - 11s 346ms/step - loss: 0.3138 - accuracy: 0.9039 - val_loss: 0.2832 - val_accuracy: 0.9165\n",
      "Epoch 8: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x218f2cb9280>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RNN_model.fit(X_train,y_train,validation_data=(X_val,y_val),epochs=100,batch_size=128,callbacks=[earlystopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now evaluate and see how the model performs on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 41ms/step - loss: 0.3117 - accuracy: 0.9045\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3117247521877289, 0.9044715166091919]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RNN_model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded vocab size 400000\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "def load_embedding_model():\n",
    "    wv_from_bin = api.load(\"glove-wiki-gigaword-200\")\n",
    "    print(\"Loaded vocab size %i\" % len(list(wv_from_bin.index_to_key)))\n",
    "    return wv_from_bin\n",
    "wv_from_bin = load_embedding_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def get_matrix_of_vectors(wv_from_bin, required_words):\n",
    "\n",
    "    words = list(wv_from_bin.index_to_key)\n",
    "    print(\"Shuffling words ...\")\n",
    "    random.seed(225)\n",
    "    random.shuffle(words)\n",
    "    words = words[:10000]\n",
    "    print(\"Putting %i words into word2ind and matrix M...\" % len(words))\n",
    "    word2ind = {}\n",
    "    M = []\n",
    "    curInd = 0\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(wv_from_bin.get_vector(w))\n",
    "            word2ind[w] = curInd\n",
    "            curInd += 1\n",
    "        except KeyError:\n",
    "            continue\n",
    "    for w in required_words:\n",
    "        if w in words:\n",
    "            continue\n",
    "        try:\n",
    "            M.append(wv_from_bin.get_vector(w))\n",
    "            word2ind[w] = curInd\n",
    "            curInd += 1\n",
    "        except KeyError:\n",
    "            continue\n",
    "    M = np.stack(M)\n",
    "    print(\"Done.\")\n",
    "    return M, word2ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "def reduce_to_k_dim(M,k=2):\n",
    "    print(f\"Reducing to {k} dimensions...\")\n",
    "    svd = TruncatedSVD(n_components=k, n_iter=10, random_state=16)\n",
    "    M_reduced = svd.fit_transform(M)\n",
    "    return M_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling words ...\n",
      "Putting 10000 words into word2ind and matrix M...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "M2,word2index2 = get_matrix_of_vectors(wv_from_bin,get_vocab(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing to 128 dimensions...\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = 100\n",
    "VOCAB_SIZE = len(get_vocab(X))\n",
    "BATCH_SIZE = 20\n",
    "M2 = reduce_to_k_dim(M2,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4915, 100)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sequences = [[word2index2.get(word, 0) for word in data_point] for data_point in X]\n",
    "padded_data = pad_sequences(data_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "padded_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(y)\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = train_test_val_split(padded_data,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_model = Sequential()\n",
    "\n",
    "vocab_size = len(word2index2)  # Add 1 for the OOV token\n",
    "CNN_model.add(Embedding(input_dim=vocab_size, output_dim=M2.shape[1], weights=[M2], input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
    "CNN_model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "CNN_model.add(MaxPooling1D(pool_size=2))\n",
    "CNN_model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "CNN_model.add(MaxPooling1D(pool_size=2))\n",
    "CNN_model.add(Flatten())\n",
    "\n",
    "# Add a Dense layer for binary classification with sigmoid activation\n",
    "CNN_model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 100, 128)          2139648   \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 96, 128)           82048     \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1  (None, 48, 128)           0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 44, 128)           82048     \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPoolin  (None, 22, 128)           0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2816)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 2817      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2306561 (8.80 MB)\n",
      "Trainable params: 166913 (652.00 KB)\n",
      "Non-trainable params: 2139648 (8.16 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "CNN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [1],\n",
       "       ...,\n",
       "       [1],\n",
       "       [0],\n",
       "       [1]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.reshape(y_train.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "197/197 [==============================] - 3s 13ms/step - loss: 8.9285e-04 - accuracy: 0.9997 - val_loss: 0.3720 - val_accuracy: 0.9145\n",
      "Epoch 2/100\n",
      "197/197 [==============================] - 2s 12ms/step - loss: 0.0012 - accuracy: 0.9997 - val_loss: 0.3871 - val_accuracy: 0.9124\n",
      "Epoch 3/100\n",
      "197/197 [==============================] - 2s 12ms/step - loss: 0.0012 - accuracy: 0.9997 - val_loss: 0.4641 - val_accuracy: 0.9145\n",
      "Epoch 4/100\n",
      "197/197 [==============================] - 2s 12ms/step - loss: 7.2337e-04 - accuracy: 1.0000 - val_loss: 0.4219 - val_accuracy: 0.9185\n",
      "Epoch 4: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2189ac4f790>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CNN_model.fit(X_train,y_train,validation_data=(X_val,y_val),epochs = 100,batch_size = BATCH_SIZE,callbacks=[earlystopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 7ms/step - loss: 0.3949 - accuracy: 0.9329\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3948810398578644, 0.9329268336296082]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CNN_model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "y_preds = CNN_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.51      0.59        47\n",
      "           1       0.95      0.98      0.96       445\n",
      "\n",
      "    accuracy                           0.93       492\n",
      "   macro avg       0.83      0.74      0.78       492\n",
      "weighted avg       0.93      0.93      0.93       492\n",
      "\n",
      "[[ 24  23]\n",
      " [ 10 435]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,f1_score,precision_score,recall_score\n",
    "\n",
    "y_preds = np.round(y_preds)\n",
    "accuracy_score(y_test,y_preds)\n",
    "print(classification_report(y_test,y_preds))\n",
    "print(confusion_matrix(y_test,y_preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
